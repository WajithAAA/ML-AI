{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Studies\\machine-learning\\end-to-end-projects\\house-price-prediction\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ELK_ENDPOINT=\"http://localhost:9200\"\n",
    "ELK_USERNAME=\"elastic\"\n",
    "ELK_PASSWORD=\"i+hOm7ofi94kXc9B3WVM\"\n",
    "ELK_INDEX='posting'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JsonlCollectionIterator:\n",
    "\n",
    "    def __init__(self, collection_path: str, fields=None, delimiter=\"\\n\"):\n",
    "        if fields:\n",
    "            self.fields = fields\n",
    "        else:\n",
    "            self.fields = ['text']\n",
    "        self.delimiter = delimiter\n",
    "        self.all_info = self._load(collection_path)\n",
    "        self.size = len(self.all_info['id'])\n",
    "        self.batch_size = 1\n",
    "        self.shard_id = 0\n",
    "        self.shard_num = 1\n",
    "\n",
    "    def __call__(self, batch_size=1, shard_id=0, shard_num=1):\n",
    "        self.batch_size = batch_size\n",
    "        self.shard_id = shard_id\n",
    "        self.shard_num = shard_num\n",
    "        return self\n",
    "\n",
    "    def __iter__(self):\n",
    "        total_len = self.size\n",
    "        shard_size = int(total_len / self.shard_num)\n",
    "        start_idx = self.shard_id * shard_size\n",
    "        end_idx = min(start_idx + shard_size, total_len)\n",
    "        if self.shard_id == self.shard_num - 1:\n",
    "            end_idx = total_len\n",
    "        to_yield = {}\n",
    "        for idx in tqdm(range(start_idx, end_idx, self.batch_size)):\n",
    "            for key in self.all_info:\n",
    "                to_yield[key] = self.all_info[key][idx: min(idx + self.batch_size, end_idx)]\n",
    "            yield to_yield\n",
    "\n",
    "    def _parse_fields_from_info(self, info):\n",
    "       \n",
    "        n_fields = len(self.fields)\n",
    "\n",
    "        if all([field in info for field in self.fields]):\n",
    "            return [info[field].strip() for field in self.fields]\n",
    "\n",
    "        assert \"contents\" in info, f\"contents not found in info: {info}\"\n",
    "        contents = info['contents']\n",
    "    \n",
    "        if contents.count(self.delimiter) == n_fields:\n",
    "            if contents.endswith(self.delimiter):\n",
    "                contents = contents[:-len(self.delimiter)]\n",
    "        return [field.strip(\" \") for field in contents.split(self.delimiter)]\n",
    "\n",
    "    def _load(self, collection_path):\n",
    "        filenames = []\n",
    "        if os.path.isfile(collection_path):\n",
    "            filenames.append(collection_path)\n",
    "        else:\n",
    "            for filename in os.listdir(collection_path):\n",
    "                filenames.append(os.path.join(collection_path, filename))\n",
    "        all_info = {field: [] for field in self.fields}\n",
    "        all_info['id'] = []\n",
    "        for filename in filenames:\n",
    "            with open(filename) as f:\n",
    "                for line_i, line in tqdm(enumerate(f)):\n",
    "                    info = json.loads(line)\n",
    "                    _id = info.get('_id', info.get('docid', None))\n",
    "                    if _id is None:\n",
    "                        raise ValueError(f\"Cannot find 'id' or 'docid' from {filename}.\")\n",
    "                    all_info['id'].append(str(_id))\n",
    "                    fields_info = self._parse_fields_from_info(info)\n",
    "                    if len(fields_info) != len(self.fields):\n",
    "                        raise ValueError(\n",
    "                            f\"{len(fields_info)} fields are found at Line#{line_i} in file {filename}.\" \\\n",
    "                            f\"{len(self.fields)} fields expected.\" \\\n",
    "                            f\"Line content: {info['contents']}\"\n",
    "                        )\n",
    "\n",
    "                    for i in range(len(fields_info)):\n",
    "                        all_info[self.fields[i]].append(fields_info[i])\n",
    "        return all_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder:\n",
    "    def __init__(self, model_name: str):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "    def encode(self, text:str, max_length: int):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**inputs, return_dict=True)\n",
    "        \n",
    "        # Perform pooling\n",
    "        embeddings = self.mean_pooling(model_output, inputs['attention_mask'])\n",
    "        # Normalize embeddings\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        return embeddings.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] \n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_indices():\n",
    "    es_client = Elasticsearch(\"https://localhost:9200\", http_auth=(ELK_USERNAME, ELK_PASSWORD),  verify_certs=False)\n",
    "    es_client.ping()\n",
    "    config = {\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"title\": {\"type\": \"text\"},\n",
    "                \"text\": {\"type\": \"text\"},\n",
    "                \"embeddings\": {\n",
    "                        \"type\": \"dense_vector\",\n",
    "                        \"dims\": 384,\n",
    "                        \"index\": True,\n",
    "                        \"similarity\": \"cosine\"\n",
    "                    }\n",
    "                }\n",
    "        },\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": 2,\n",
    "            \"number_of_replicas\": 1\n",
    "        }\n",
    "    }\n",
    "\n",
    "    es_client.indices.create(\n",
    "        index=\"vector-search-doc\",\n",
    "        settings=config[\"settings\"],\n",
    "        mappings=config[\"mappings\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search(query: str, es_client: Elasticsearch, model: str, index: str, top_k: int = 10):\n",
    "\n",
    "    encoder = Encoder(model)\n",
    "    query_vector = encoder.encode(query, max_length=64)\n",
    "    query_dict = {\n",
    "        \"field\": \"embeddings\",\n",
    "        \"query_vector\": query_vector[0].tolist(),\n",
    "        \"k\": 10,\n",
    "        \"num_candidates\": top_k\n",
    "    }\n",
    "    res = es_client.knn_search(index=index, knn=query_dict, source=[\"title\", \"text\", \"id\"])\n",
    "\n",
    "    for hit in res[\"hits\"][\"hits\"]:\n",
    "        print(hit)\n",
    "        print(f\"Document ID: {hit['_id']}\")\n",
    "        print(f\"Document Title: {hit['_source']['title']}\")\n",
    "        print(f\"Document Text: {hit['_source']['text']}\")\n",
    "        print(\"=====================================================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_path = ''\n",
    "collection_iterator = JsonlCollectionIterator(collection_path, fields=['title','text'])\n",
    "encoder = Encoder('sentence-transformers/msmarco-MiniLM-L6-cos-v5')\n",
    "es_client = Elasticsearch(\"https://localhost:9200\", http_auth=(ELK_USERNAME, ELK_PASSWORD),  verify_certs=False)\n",
    "index_name = \"msmarco-demo\"\n",
    "\n",
    "for batch_info in collection_iterator(batch_size=64, shard_id=0, shard_num=1):\n",
    "    embeddings = encoder.encode(batch_info['text'], 512)\n",
    "    batch_info[\"dense_vectors\"] = embeddings\n",
    "\n",
    "    actions = []\n",
    "    for i in range(len(batch_info['id'])):\n",
    "        action = {\"index\": {\"_index\": index_name, \"_id\": batch_info['id'][i]}}\n",
    "        doc = {\n",
    "                \"title\": batch_info['title'][i],\n",
    "                \"text\": batch_info['text'][i],\n",
    "                \"embeddings\": batch_info['dense_vectors'][i].tolist()\n",
    "            }\n",
    "        actions.append(action)\n",
    "        actions.append(doc)\n",
    "        \n",
    "        es_client.bulk(index=index_name, operations=actions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1274596509647a83bca527aa23e41897ce0742fee0f658262bbdd4d4a2eef0c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
